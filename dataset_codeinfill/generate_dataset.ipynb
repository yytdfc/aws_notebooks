{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5d8a54-79a8-4f6f-b79f-a776ccf34ddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset preparing guide for code infilling task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d5cbf-0a7e-4280-8283-5c0bab83c6f1",
   "metadata": {},
   "source": [
    "Prepare dataset for deepseek-coder model training\n",
    "\n",
    "- Deepseek-coder Model card: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base\n",
    "- LLama factory dataset format: https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html\n",
    "- Model Hub: https://github.com/aws-samples/llm_model_hub\n",
    "- Reference blog: https://aws.amazon.com/cn/blogs/china/yxt-innovative-practice-of-fine-tuning-large-models-and-enabling-code-generation-based-on-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b776755-359a-468f-b058-34edf0246e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart_open in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (7.0.5)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: tree_sitter in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.23.2)\n",
      "Requirement already satisfied: tree_sitter_python in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.23.2)\n",
      "Requirement already satisfied: tree_sitter_java in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.23.2)\n",
      "Requirement already satisfied: tree_sitter_javascript in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: tree_sitter_cpp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from smart_open) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->datasets) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install smart_open datasets tree_sitter tree_sitter_python tree_sitter_java tree_sitter_javascript tree_sitter_cpp transformers\n",
    "# you need to login to \"huggingface-cli login\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e02bf-09c5-42ef-a027-2bc813084127",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download sample dataset\n",
    "\n",
    "Here download the bigcode/the-stack-v2-train-smol-ids sample dataset for example, you can prepare your own code data in code directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aefe655a-d37b-474c-81b4-10e3e0c2da2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0552e6345f764b78aaea20083fbf0d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38a08ac15c34e0badcc546df20f4a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 775/1000 [04:53<20:00,  5.34s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from smart_open import open\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "session = boto3.Session()\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def download_contents(files):\n",
    "    for file in files:\n",
    "        s3_url = f\"s3://softwareheritage/content/{file['blob_id']}\"\n",
    "        with open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "            file[\"content\"] = fin.read().decode(file[\"src_encoding\"])\n",
    "    \n",
    "    return {\"files\": files}\n",
    "\n",
    "ds = load_dataset(\"bigcode/the-stack-v2-train-smol-ids\", split=\"train\", streaming=True)\n",
    "ds = ds.map(lambda row: download_contents(row[\"files\"]))\n",
    "filetype = {\n",
    "    \"java\",\n",
    "    \"js\",\n",
    "    \"py\",\n",
    "}\n",
    "\n",
    "import os\n",
    "os.makedirs(\"code\", exist_ok=True) \n",
    "\n",
    "\n",
    "total_count = 1000\n",
    "count = 0\n",
    "progress = tqdm(total=total_count)\n",
    "for row in ds:\n",
    "    for file in row[\"files\"]:\n",
    "        filename = file[\"path\"].split(\"/\")[-1]\n",
    "        file_suffix = filename.split(\".\")[-1]\n",
    "        if file_suffix not in filetype:\n",
    "            continue\n",
    "        open(f\"code/{file['content_id']}.{file_suffix}\", \"w\").write(file[\"content\"])\n",
    "        if count > total_count:\n",
    "            break\n",
    "        count += 1\n",
    "        progress.update(1)\n",
    "    if count > total_count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd47dff-53a0-43eb-849d-d2cd081c560a",
   "metadata": {},
   "source": [
    "## Preprocess code\n",
    "1. Preprocessing code by doing code AST\n",
    "2. Split code to prefix + middle + suffix randomly\n",
    "3. Apply prompt template to Deepseek-Coder format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08cbf7e-938f-4666-8eb0-bebaa117be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from code_splitter import split_code_randomly\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-base\")\n",
    "\n",
    "\n",
    "def read_files(directory):\n",
    "    # Only process .java and .py files\n",
    "    patterns = ['*.java', '*.py', '*.js']\n",
    "    files = []\n",
    "    for pattern in patterns:\n",
    "        files.extend(glob.glob(os.path.join(directory, pattern)))\n",
    "    \n",
    "    for file_path in files:\n",
    "        # Skip directories\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                yield file.read(), os.path.basename(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "def code_to_text(prefix, middle, suffix):\n",
    "    input_text = f\"<｜fim▁begin｜>{prefix}<｜fim▁hole｜>{suffix}<｜fim▁end｜>{middle}<｜end▁of▁sentence｜>\"\n",
    "    return {\"text\": input_text}\n",
    "    # return json_line\n",
    "\n",
    "def test_split_code_randomly():\n",
    "    directory = os.path.abspath(os.path.join(os.path.dirname(__file__), '../the-stack-v2-train-smol-ids', 'code'))\n",
    "    for code, file_name in read_files(directory):\n",
    "        print(f\"Testing file: {file_name}\")\n",
    "        \n",
    "        # Test with default parameters\n",
    "        result = split_code_randomly(code, language=file_name.split(\".\")[-1])\n",
    "        \n",
    "        # Test with custom parameters\n",
    "        min_length = random.randint(1, 10)\n",
    "        max_length = random.randint(16, 64)\n",
    "        result = split_code_randomly(code, language='java', min_middle_length=min_length, max_middle_length=max_length)\n",
    "        assert len(result['prefix']) + len(result['middle']) + len(result['suffix']) == len(code)\n",
    "        assert result['prefix'] + result['middle'] + result['suffix'] == code\n",
    "        \n",
    "        print(f\"  Prefix length: {len(result['prefix'])}\")\n",
    "        print(f\"  Middle length: {len(result['middle'])}\")\n",
    "        print(f\"  Suffix length: {len(result['suffix'])}\")\n",
    "        print(\"  Test passed!\")\n",
    "        print()\n",
    "\n",
    "def code_to_dataset(code_path, output_path, min_middle_length=1, max_middle_length=64, splits_per_file=10, min_tokens=4, max_tokens=32768):\n",
    "    \"\"\"\n",
    "    Convert code files to a dataset of split code in jsonlines format.\n",
    "\n",
    "    Args:\n",
    "        code_path: Directory containing source code files\n",
    "        output_path: Path to write the output jsonlines file\n",
    "        min_middle_length: Minimum length of the middle section\n",
    "        max_middle_length: Maximum length of the middle section\n",
    "        splits_per_file: Number of different splits to generate for each file\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    datasets = []\n",
    "    tokens = 0\n",
    "    # Process each code file\n",
    "    for code, file_name in tqdm(read_files(code_path)):\n",
    "        try:\n",
    "            # Get file extension (language)\n",
    "            language = file_name.split(\".\")[-1]\n",
    "            \n",
    "\n",
    "            # Generate multiple splits for each file\n",
    "            for i in range(splits_per_file):\n",
    "                # Split the code\n",
    "                split_result = split_code_randomly(\n",
    "                    code,\n",
    "                    language=language,\n",
    "                    min_middle_length=min_middle_length,\n",
    "                    max_middle_length=max_middle_length\n",
    "                )\n",
    "\n",
    "                assert split_result['prefix'] + split_result['middle'] + split_result['suffix'] == code\n",
    "\n",
    "                # Convert to text format and write to file\n",
    "                json_line = code_to_text(\n",
    "                    split_result['prefix'],\n",
    "                    split_result['middle'],\n",
    "                    split_result['suffix']\n",
    "                )\n",
    "\n",
    "                token_count = tokenizer(json_line[\"text\"], return_tensors=\"pt\").input_ids.shape[1]\n",
    "                if token_count > max_tokens or token_count < min_tokens:\n",
    "                    print(f\"Skip {file_name}, token count {token_count}\")\n",
    "                    break\n",
    "                tokens += token_count\n",
    "                datasets.append(json_line)\n",
    "            else:\n",
    "                # print(f\"Generated {splits_per_file} splits for {file_name}\")\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "            continue\n",
    "    print(f\"average tokens {tokens / len(datasets):.2f}\")\n",
    "    json.dump(datasets, open(output_path, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623920e5-b5a0-47bc-b2f0-bc35b65c6c89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 27.77it/s]\u001b[A\n",
      "11it [00:00, 47.23it/s]\u001b[A\n",
      "18it [00:00, 52.90it/s]\u001b[A\n",
      "24it [00:00, 53.51it/s]\u001b[A\n",
      "32it [00:00, 52.32it/s]\u001b[A\n",
      "38it [00:00, 36.30it/s]\u001b[A\n",
      "43it [00:01, 36.41it/s]\u001b[A\n",
      "52it [00:01, 46.35it/s]\u001b[A\n",
      "58it [00:01, 38.52it/s]\u001b[A\n",
      "65it [00:01, 43.41it/s]\u001b[A\n",
      "70it [00:01, 34.93it/s]\u001b[A\n",
      "75it [00:01, 34.33it/s]\u001b[A\n",
      "83it [00:01, 43.57it/s]\u001b[A\n",
      "89it [00:02, 33.53it/s]\u001b[A\n",
      "95it [00:02, 38.34it/s]\u001b[A\n",
      "101it [00:02, 42.55it/s]\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (29957 > 16384). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "107it [00:03, 12.97it/s]\u001b[A\n",
      "115it [00:03, 18.18it/s]\u001b[A\n",
      "122it [00:03, 23.36it/s]\u001b[A\n",
      "128it [00:04, 25.43it/s]\u001b[A\n",
      "133it [00:04, 26.05it/s]\u001b[A\n",
      "141it [00:04, 34.08it/s]\u001b[A\n",
      "150it [00:04, 44.00it/s]\u001b[A\n",
      "157it [00:04, 42.98it/s]\u001b[A\n",
      "165it [00:04, 45.40it/s]\u001b[A\n",
      "171it [00:04, 47.36it/s]\u001b[A\n",
      "181it [00:05, 58.71it/s]\u001b[A\n",
      "188it [00:05, 25.32it/s]\u001b[A\n",
      "195it [00:05, 30.69it/s]\u001b[A\n",
      "206it [00:05, 42.08it/s]\u001b[A\n",
      "213it [00:06, 36.81it/s]\u001b[A\n",
      "219it [00:06, 36.88it/s]\u001b[A\n",
      "225it [00:06, 36.28it/s]\u001b[A\n",
      "233it [00:06, 43.40it/s]\u001b[A\n",
      "239it [00:06, 36.90it/s]\u001b[A\n",
      "246it [00:06, 42.57it/s]\u001b[A\n",
      "252it [00:07, 43.25it/s]\u001b[A\n",
      "258it [00:07, 45.77it/s]\u001b[A\n",
      "264it [00:07, 45.81it/s]\u001b[A\n",
      "272it [00:07, 52.89it/s]\u001b[A\n",
      "278it [00:07, 48.56it/s]\u001b[A\n",
      "284it [00:07, 50.75it/s]\u001b[A\n",
      "292it [00:07, 57.70it/s]\u001b[A\n",
      "299it [00:07, 54.43it/s]\u001b[A\n",
      "305it [00:08, 48.65it/s]\u001b[A\n",
      "312it [00:08, 52.37it/s]\u001b[A\n",
      "318it [00:08, 52.41it/s]\u001b[A\n",
      "326it [00:08, 58.33it/s]\u001b[A\n",
      "333it [00:08, 42.82it/s]\u001b[A\n",
      "339it [00:09, 31.86it/s]\u001b[A\n",
      "344it [00:09, 32.69it/s]\u001b[A\n",
      "348it [00:09, 29.45it/s]\u001b[A\n",
      "356it [00:09, 38.11it/s]\u001b[A\n",
      "361it [00:09, 26.99it/s]\u001b[A\n",
      "368it [00:09, 31.75it/s]\u001b[A\n",
      "374it [00:10, 31.86it/s]\u001b[A\n",
      "378it [00:10, 32.09it/s]\u001b[A\n",
      "382it [00:10, 31.63it/s]\u001b[A\n",
      "387it [00:10, 30.20it/s]\u001b[A\n",
      "393it [00:10, 35.53it/s]\u001b[A\n",
      "400it [00:10, 43.11it/s]\u001b[A\n",
      "405it [00:11, 13.74it/s]\u001b[A\n",
      "415it [00:11, 21.85it/s]\u001b[A\n",
      "425it [00:12, 30.86it/s]\u001b[A\n",
      "432it [00:12, 34.27it/s]\u001b[A\n",
      "441it [00:12, 42.23it/s]\u001b[A\n",
      "448it [00:12, 24.08it/s]\u001b[A\n",
      "453it [00:13, 26.69it/s]\u001b[A\n",
      "459it [00:13, 29.73it/s]\u001b[A\n",
      "464it [00:13, 24.97it/s]\u001b[A\n",
      "470it [00:13, 30.15it/s]\u001b[A\n",
      "476it [00:13, 33.62it/s]\u001b[A\n",
      "481it [00:13, 34.28it/s]\u001b[A\n",
      "486it [00:13, 37.20it/s]\u001b[A\n",
      "491it [00:14, 14.57it/s]\u001b[A\n",
      "500it [00:15, 21.23it/s]\u001b[A\n",
      "509it [00:15, 29.65it/s]\u001b[A\n",
      "515it [00:15, 34.19it/s]\u001b[A\n",
      "522it [00:15, 35.89it/s]\u001b[A\n",
      "529it [00:15, 41.95it/s]\u001b[A\n",
      "535it [00:15, 37.76it/s]\u001b[A\n",
      "540it [00:16, 24.94it/s]\u001b[A\n",
      "545it [00:16, 27.89it/s]\u001b[A\n",
      "552it [00:16, 34.55it/s]\u001b[A\n",
      "557it [00:16, 32.71it/s]\u001b[A\n",
      "562it [00:16, 29.99it/s]\u001b[A\n",
      "566it [00:17, 20.94it/s]\u001b[A\n",
      "570it [00:17, 22.78it/s]\u001b[A\n",
      "573it [00:17, 23.73it/s]\u001b[A\n",
      "578it [00:17, 27.40it/s]\u001b[A\n",
      "582it [00:17, 28.72it/s]\u001b[A\n",
      "587it [00:17, 31.57it/s]\u001b[A\n",
      "591it [00:17, 27.88it/s]\u001b[A\n",
      "596it [00:18, 31.36it/s]\u001b[A\n",
      "603it [00:18, 39.42it/s]\u001b[A\n",
      "609it [00:18, 43.40it/s]\u001b[A\n",
      "614it [00:18, 40.78it/s]\u001b[A\n",
      "619it [00:18, 32.93it/s]\u001b[A\n",
      "624it [00:18, 34.14it/s]\u001b[A\n",
      "628it [00:18, 29.86it/s]\u001b[A\n",
      "632it [00:19, 31.48it/s]\u001b[A\n",
      "640it [00:19, 42.36it/s]\u001b[A\n",
      "645it [00:19, 42.30it/s]\u001b[A\n",
      "650it [00:19, 38.58it/s]\u001b[A\n",
      "656it [00:19, 43.37it/s]\u001b[A\n",
      "661it [00:19, 34.79it/s]\u001b[A\n",
      "666it [00:20, 15.83it/s]\u001b[A\n",
      "670it [00:20, 17.25it/s]\u001b[A\n",
      "673it [00:21, 11.12it/s]\u001b[A\n",
      "680it [00:21, 16.61it/s]\u001b[A\n",
      "684it [00:21, 17.98it/s]\u001b[A\n",
      "691it [00:21, 25.03it/s]\u001b[A\n",
      "696it [00:22, 19.23it/s]\u001b[A\n",
      "700it [00:22, 19.24it/s]\u001b[A\n",
      "709it [00:22, 26.94it/s]\u001b[A\n",
      "713it [00:22, 26.22it/s]\u001b[A\n",
      "717it [00:23, 18.46it/s]\u001b[A\n",
      "722it [00:23, 22.63it/s]\u001b[A\n",
      "726it [00:23, 23.03it/s]\u001b[A\n",
      "730it [00:23, 24.38it/s]\u001b[A\n",
      "733it [00:23, 22.46it/s]\u001b[A\n",
      "736it [00:23, 21.43it/s]\u001b[A\n",
      "741it [00:23, 27.06it/s]\u001b[A\n",
      "748it [00:24, 34.08it/s]\u001b[A\n",
      "752it [00:24, 33.82it/s]\u001b[A\n",
      "758it [00:24, 38.79it/s]\u001b[A\n",
      "765it [00:24, 33.23it/s]\u001b[A\n",
      "769it [00:24, 31.10it/s]\u001b[A\n",
      "773it [00:24, 31.57it/s]\u001b[A\n",
      "777it [00:24, 31.39it/s]\u001b[A\n",
      "781it [00:25, 32.42it/s]\u001b[A\n",
      "785it [00:25, 17.85it/s]\u001b[A\n",
      "793it [00:25, 25.66it/s]\u001b[A\n",
      "799it [00:25, 30.25it/s]\u001b[A\n",
      "803it [00:26, 22.68it/s]\u001b[A\n",
      "807it [00:26, 23.44it/s]\u001b[A\n",
      "812it [00:26, 27.28it/s]\u001b[A\n",
      "816it [00:26, 22.36it/s]\u001b[A\n",
      "819it [00:26, 19.51it/s]\u001b[A\n",
      "822it [00:27, 15.06it/s]\u001b[A\n",
      "826it [00:27, 15.33it/s]\u001b[A\n",
      "833it [00:27, 22.79it/s]\u001b[A\n",
      "837it [00:27, 24.34it/s]\u001b[A\n",
      "841it [00:27, 26.54it/s]\u001b[A\n",
      "845it [00:27, 26.12it/s]\u001b[A\n",
      "849it [00:28, 17.28it/s]\u001b[A\n",
      "855it [00:28, 23.04it/s]\u001b[A\n",
      "859it [00:28, 21.73it/s]\u001b[A\n",
      "863it [00:29, 17.48it/s]\u001b[A\n",
      "869it [00:29, 23.07it/s]\u001b[A\n",
      "873it [00:29, 24.60it/s]\u001b[A\n",
      "879it [00:29, 30.94it/s]\u001b[A\n",
      "883it [00:29, 16.90it/s]\u001b[A\n",
      "891it [00:30, 24.90it/s]\u001b[A\n",
      "896it [00:30, 26.78it/s]\u001b[A\n",
      "900it [00:30, 23.97it/s]\u001b[A\n",
      "904it [00:31,  9.02it/s]\u001b[A\n",
      "910it [00:31, 12.73it/s]\u001b[A\n",
      "914it [00:32, 13.56it/s]\u001b[A\n",
      "922it [00:32, 20.14it/s]\u001b[A\n",
      "926it [00:32, 22.02it/s]\u001b[A\n",
      "930it [00:32, 23.00it/s]\u001b[A\n",
      "934it [00:32, 25.46it/s]\u001b[A\n",
      "939it [00:32, 30.04it/s]\u001b[A\n",
      "945it [00:32, 36.29it/s]\u001b[A\n",
      "951it [00:33, 29.89it/s]\u001b[A\n",
      "955it [00:33, 27.96it/s]\u001b[A\n",
      "961it [00:33, 33.05it/s]\u001b[A\n",
      "965it [00:33, 34.31it/s]\u001b[A\n",
      "969it [00:33, 30.76it/s]\u001b[A\n",
      "974it [00:33, 34.10it/s]\u001b[A\n",
      "978it [00:33, 29.08it/s]\u001b[A\n",
      "982it [00:34, 31.06it/s]\u001b[A\n",
      "986it [00:34, 26.95it/s]\u001b[A\n",
      "992it [00:34, 33.77it/s]\u001b[A\n",
      "1002it [00:34, 28.98it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average tokens 925.52\n",
      "Dataset generation complete\n"
     ]
    }
   ],
   "source": [
    "# Current directory as code path\n",
    "code_path = \"./code\"  # Current directory containing source code files\n",
    "output_path = \"./code_dataset.json\"  # Output jsonlines file\n",
    "\n",
    "# Create dataset with custom parameters\n",
    "code_to_dataset(\n",
    "    code_path=code_path,\n",
    "    output_path=output_path,\n",
    "    min_middle_length=1,\n",
    "    max_middle_length=64,\n",
    "    splits_per_file=5,  # Generate 5 different splits for each file\n",
    "    max_tokens=32768,  # Max tokens per file, this should be considered with \"cutoff_len\" parameter in LLamaFactory\n",
    ")\n",
    "print(\"Dataset generation complete\")\n",
    "\n",
    "json.dump({\n",
    "    \"code_dataset\":{\n",
    "        \"file_name\": output_path,\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"text\"\n",
    "        }\n",
    "    }\n",
    "}, open(\"./dataset_info.json\", 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f44568-d1c5-44bd-b793-0e6274d208f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare for training\n",
    "\n",
    "You can start training job on SageMaker, example: https://github.com/xqun3/Training_On_SageMaker\n",
    "\n",
    "or via Modelhub (based on LLamaFactory): https://github.com/aws-samples/llm_model_hub\n",
    "\n",
    "The dataset info file for LLamaFactory is like this:\n",
    "\n",
    "``` json\n",
    "{\n",
    "    \"code_dataset\":{\n",
    "        \"file_name\":\"code_dataset.json\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"text\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
