{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with LLaMA-Factory on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install once\n",
    "# !pip install -U boto3 sagemaker awscli modelscope huggingface-hub\n",
    "# restart jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare dataset\n",
    "\n",
    "You can prepare your finetune dataset in format according to:\n",
    "[https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html)\n",
    "\n",
    "Here is an example dataset `llamafactory/PubMedQA` on HuggingFace, and push to s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "dataset_repo_id = \"llamafactory/PubMedQA\"\n",
    "local_dataset_path = f\"/home/ec2-user/dataset/{dataset_repo_id}\"\n",
    "\n",
    "repo_path = snapshot_download(\n",
    "    repo_id=dataset_repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=local_dataset_path, \n",
    ")\n",
    "\n",
    "s3_dataset_path = f's3://{sagemaker_default_bucket}/dataset/{dataset_repo_id}'\n",
    "\n",
    "!aws s3 sync {local_dataset_path} {s3_dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare pretrained model\n",
    "\n",
    "Download pretrain models and push to s3 bucket. Here using Qwen2.5 model from modelscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "local_model_path = f\"/home/ec2-user/model/{model_id}\"\n",
    "s3_model_path = f's3://{sagemaker_default_bucket}/pretrained-models/{model_id}'\n",
    "\n",
    "!modelscope download --local_dir {local_model_path} {model_id} \n",
    "!aws s3 sync {local_model_path} {s3_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare training config\n",
    "\n",
    "Prepare training script in `submit_src`, the entrypoint is `submit_src/estimator_entry.py`, training config is `.yaml`.\n",
    "More about llamafactory training config: https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/sft.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git -b v0.9.1 ./submit_src/LLaMA-Factory && rm -rf ./submit_src/LLaMA-Factory/.git\n",
    "!curl -L https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz | tar -xz s5cmd && mv s5cmd ./submit_src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llamafactory_yaml = \"qwen_full_sft\"\n",
    "llamafactory_yaml = \"qwen_lora_sft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start training on SageMaker training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "if region.startswith(\"us-\"):\n",
    "    image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "elif region.startswith(\"cn-\"):\n",
    "    image_uri = f'727897471807.dkr.ecr.{region}.amazonaws.com.cn/pytorch-training:2.3.0-gpu-py311-cu121-ubuntu20.04-sagemaker'\n",
    "else:\n",
    "    raise Exception(\"find image_uri on more images from https://github.com/aws/deep-learning-containers/blob/master/available_images.md\")\n",
    "    \n",
    "# instance_type = \"ml.g5.2xlarge\"     # 1 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.g5.12xlarge\"    # 4 * A10g (24G/GPU)\n",
    "instance_type = \"ml.g5.48xlarge\"    # 8 * A10g (24G/GPU)\n",
    "# instance_type = \"ml.p4d.24xlarge\"   # 8 * A100 (40G/GPU)\n",
    "# instance_type = \"ml.p5.48xlarge\"    # 8 * H100 (80G/GPU)\n",
    "# instance_type = \"ml.g6e.48xlarge\"   # 8 * L40s (80G/GPU)\n",
    "\n",
    "instance_count = 1                  # 1 or Multi-node\n",
    "\n",
    "envs = {\n",
    "    'MODEL_ID_OR_S3_PATH': f\"{s3_model_path}/*\",\n",
    "    'MODEL_SAVE_PATH_S3': f's3://{sagemaker_default_bucket}/output-model/250110/{model_id}',\n",
    "    'CONF_YAML_NAME': f'{llamafactory_yaml}.yaml',\n",
    "    'DATASET_S3_PATH': f\"{s3_dataset_path}/*\",\n",
    "}\n",
    "\n",
    "hypers = {\n",
    "}\n",
    "\n",
    "base_job_name = f\"{model_id}-{llamafactory_yaml}\".replace('/','-').replace('.','-').replace('_','-')\n",
    "\n",
    "smp_estimator = Estimator(\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name=base_job_name,\n",
    "    entry_point=\"estimator_entry.py\",\n",
    "    source_dir='submit_src/',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    environment=envs,\n",
    "    hyperparameters=hypers,\n",
    "    image_uri=image_uri,\n",
    "    max_run=7200,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    disable_output_compression=True,\n",
    ")\n",
    "\n",
    "job_name = sagemaker.utils.name_from_base(base_job_name, short=True)\n",
    "\n",
    "smp_estimator.fit(job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed test\n",
    "\n",
    "Training speed test on g5 instances:\n",
    "\n",
    "|                       | Qwen2.5-32B-Instruct | Qwen2.5-32B-Instruct |\n",
    "| --------------------- | -------------------- | -------------------- |\n",
    "| finetune_type         | **full**             | **lora**             |\n",
    "| deepspeed             | zero3-offload        | zero3-offload        |\n",
    "| instance              | g5.48xlarge          | g5.48xlarge          |\n",
    "| instance_num          | **2**                | **1**                |\n",
    "| batch_size_per_device | 4                    | 4                    |\n",
    "| cutoff_len            | 2048                 | 2048                 |\n",
    "| gradient_accumulation | 1                    | 1                    |\n",
    "| total_batch_size      | 64                   | 32                   |\n",
    "| seconds_per_batch     | **89**               | **72**               |\n",
    "| samples_per_hour      | **2588.8**           | **1600.0**           |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
