{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Create your container repository\n",
    "\n",
    "open aws console and create a repository for your container: https://us-west-2.console.aws.amazon.com/ecr/create-repository?region=us-west-2\n",
    "\n",
    "for example `236995464743.dkr.ecr.us-west-2.amazonaws.com/sagemaker_endpoint/vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "# login\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 236995464743.dkr.ecr.us-west-2.amazonaws.com\n",
    "\n",
    "VLLM_VERSION = \"v0.5.4\"\n",
    "REPO_NAME = \"sagemaker_endpoint/vllm\"\n",
    "CONTAINER = f\"236995464743.dkr.ecr.us-west-2.amazonaws.com/{REPO_NAME}:{VLLM_VERSION}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "demo codes are in `app/`\n",
    "build and push the docker with following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   51.2kB\n",
      "Step 1/9 : ARG VLLM_VERSION\n",
      "Step 2/9 : FROM vllm/vllm-openai:$VLLM_VERSION\n",
      " ---> 28a486908227\n",
      "Step 3/9 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 26cd4f6c1062\n",
      "Step 4/9 : RUN sed -i 's|/v1/chat/completions|/invocations|g' /usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py;     sed -i 's|/health|/ping|g' /usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py;\n",
      " ---> Using cache\n",
      " ---> a0213e02b6ca\n",
      "Step 5/9 : COPY app/ /app\n",
      " ---> Using cache\n",
      " ---> 3bc86774b549\n",
      "Step 6/9 : EXPOSE 8080\n",
      " ---> Using cache\n",
      " ---> b46a7dab5ab2\n",
      "Step 7/9 : ENV PATH=\"/app:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 179b301761c6\n",
      "Step 8/9 : ENTRYPOINT []\n",
      " ---> Running in 5dbba2abcdb1\n",
      "Removing intermediate container 5dbba2abcdb1\n",
      " ---> e31beea64c81\n",
      "Step 9/9 : CMD [\"serve\"]\n",
      " ---> Running in c706e5e7532c\n",
      "Removing intermediate container c706e5e7532c\n",
      " ---> 6cf4c83dc5c9\n",
      "Successfully built 6cf4c83dc5c9\n",
      "Successfully tagged sagemaker_endpoint/vllm:v0.5.4\n",
      "The push refers to repository [236995464743.dkr.ecr.us-west-2.amazonaws.com/sagemaker_endpoint/vllm]\n",
      "\n",
      "\u001b[1B445cc516: Preparing \n",
      "\u001b[1B4a43ee6f: Preparing \n",
      "\u001b[1B29833832: Preparing \n",
      "\u001b[1B2aa93bc4: Preparing \n",
      "\u001b[1Bc00e53e7: Preparing \n",
      "\u001b[1B71044cf3: Preparing \n",
      "\u001b[1B242cb3b1: Preparing \n",
      "\u001b[1Bb8a7c409: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B4bd033dc: Preparing \n",
      "\u001b[1B78734bd5: Preparing \n",
      "\u001b[1B9b6d1f21: Preparing \n",
      "\u001b[1B335e5a99: Preparing \n",
      "\u001b[1B3bb9c80f: Preparing \n",
      "\u001b[1Bffc45974: Preparing \n",
      "\u001b[1Bf7829cb7: Preparing \n",
      "\u001b[1B869b72ab: Preparing \n",
      "\u001b[2B869b72ab: Layer already exists \u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2Kv0.5.4: digest: sha256:0d3512baf64e03fd1c4a1ae53ff6efa1b169d759e3ee9e732be28de84c462c01 size: 4101\n"
     ]
    }
   ],
   "source": [
    "!docker build --build-arg VLLM_VERSION={VLLM_VERSION} -t {REPO_NAME}:{VLLM_VERSION} .\n",
    "!docker tag {REPO_NAME}:{VLLM_VERSION} {CONTAINER}\n",
    "!docker push {CONTAINER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: deploy vllm by scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the entrypoint of the endpoint is start.sh\n",
      "====================================================\n",
      "#!/bin/bash\n",
      "\n",
      "# port needs to be 8080\n",
      "\n",
      "python3 -m vllm.entrypoints.openai.api_server \\\n",
      "    --port 8080 \\\n",
      "    --trust-remote-code \\\n",
      "    --model deepseek-ai/deepseek-coder-1.3b-instruct\n",
      "====================================================\n",
      "vllm_by_scripts/\n",
      "vllm_by_scripts/start.sh\n",
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-236995464743/sagemaker_endpoint/vllm//vllm_by_scripts.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!echo the entrypoint of the endpoint is \"start.sh\"\n",
    "!echo ====================================================\n",
    "!cat vllm_by_scripts/start.sh\n",
    "!echo ====================================================\n",
    "\n",
    "!rm vllm_by_scripts.tar.gz\n",
    "!tar czvf vllm_by_scripts.tar.gz vllm_by_scripts/\n",
    "\n",
    "\n",
    "s3_code_prefix = f\"sagemaker_endpoint/vllm/\"\n",
    "bucket = sess.default_bucket() \n",
    "code_artifact = sess.upload_data(\"vllm_by_scripts.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: deploy vllm by model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write the model_id to file model_id\n",
      "====================================================\n",
      "deepseek-ai/deepseek-coder-1.3b-instruct\n",
      "====================================================\n",
      "\n",
      "write envs to file .env\n",
      "====================================================\n",
      "# Environment Variables: https://docs.vllm.ai/en/latest/serving/env_vars.html\n",
      "export HF_TOKEN=\"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\"\n",
      "====================================================\n",
      "vllm_by_model_id/\n",
      "vllm_by_model_id/.env\n",
      "vllm_by_model_id/model_id\n",
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-236995464743/sagemaker_endpoint/vllm//vllm_by_model_id.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!echo write the model_id to file \"model_id\"\n",
    "!echo ====================================================\n",
    "!cat vllm_by_model_id/model_id\n",
    "!echo ====================================================\n",
    "!echo \n",
    "!echo write envs to file \".env\"\n",
    "!echo ====================================================\n",
    "!cat vllm_by_model_id/.env\n",
    "!echo ====================================================\n",
    "\n",
    "!rm vllm_by_model_id.tar.gz\n",
    "!tar czvf vllm_by_model_id.tar.gz vllm_by_model_id/\n",
    "\n",
    "\n",
    "s3_code_prefix = f\"sagemaker_endpoint/vllm/\"\n",
    "bucket = sess.default_bucket() \n",
    "code_artifact = sess.upload_data(\"vllm_by_model_id.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: sagemaker-vllm-2024-08-29-10-10-27-438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: sagemaker-vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    name=\"sagemaker-vllm\",\n",
    "    model_data=code_artifact,\n",
    "    image_uri=CONTAINER,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# 部署模型到endpoint\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-vllm\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "you can invoke your model with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a basic implementation of Quick Sort in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[0]\n",
      "        less_than_pivot = [x for x in arr[1:] if x <= pivot]\n",
      "        greater_than_pivot = [x for x in arr[1:] if x > pivot]\n",
      "        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "This implementation works by choosing the first element of the list as the pivot, and then dividing the rest of the list into two lists, one with elements less than the pivot and one with elements greater than the pivot. The pivot is then in its final position in the sorted list. The function is then recursively called on the two lists.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Testing the function\n",
      "print(quicksort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "In this code, we first check if the length of the array is less than or equal to 1, in which case the array is already sorted, so we return it directly. If the array is longer, we choose a pivot element from the middle of the array. We then create three lists: one for elements less than the pivot, one for elements equal to the pivot, and one for elements greater than the pivot. We then recursively sort the elements less than the pivot and those greater than the pivot, and concatenate the results with the elements equal to the pivot.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "for t in response['Body']:\n",
    "    try:\n",
    "        data = json.loads(re.sub(r\"^data: \", \"\", t[\"PayloadPart\"][\"Bytes\"].decode())) \n",
    "        print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
